{
  "body_html": "\n\n  \n    关于elasticsearch脑裂问题\n    \n    \n    \n    \n    \n  <link href=\"https://www.yuque.com/attachments/yuque/0/2020/css/491128/1588213910235-976bcd3a-7a9c-473d-b800-1b1df54807d4.css\" rel=\"stylesheet\" type=\"text/css\" />\n<link href=\"https://www.yuque.com/attachments/yuque/0/2020/css/491128/1588213910286-64db0dd0-ac29-46fb-9241-76ef318b1b53.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n  \n        \n<div class=\"page\">\n    \n        <h1 class=\"book-chapter\" id=\"calibre_toc_38\">关于elasticsearch脑裂问题</h1>\n        <div class=\"section\">\n            <h1 id=\"elasticsearch脑裂问题\" class=\"calibre7\">elasticsearch脑裂问题</h1>\n<p class=\"calibre12\">所谓脑裂问题（类似于精神分裂），就是同一个集群中的不同节点，对于集群的状态有了不一样的理解。</p>\n<p class=\"calibre12\">情况描述，通过以下命令查看集群状态：</p>\n<pre class=\"calibre10\"><code class=\"pcalibre4 pcalibre3 calibre11\">curl -XGET 'es-1:9200/_cluster/health'\n</code></pre><p class=\"calibre12\">发现，集群的总体状态是red，本来9个节点的集群，在结果中只显示了4个；但是，将请求发向不同的节点之后，我却发现即使是总体状态是red的，但是可用的节点数量却不一致。</p>\n<p class=\"calibre12\">正常情况下，集群中的所有的节点，应该对集群中master的选择是一致的，这样获得的状态信息也应该是一致的，不一致的状态信息，说明不同的节点对master节点的选择出现了异常——也就是所谓的脑裂问题。这样的脑裂状态直接让节点失去了集群的正确状态，导致集群不能正常工作。</p>\n<h3 id=\"可能导致的原因：\" class=\"calibre9\">可能导致的原因：</h3>\n<h4 id=\"1-网络原因：\" class=\"calibre15\">1. 网络原因：</h4>\n<p class=\"calibre12\">由于是内网通信，网络通信问题造成某些节点认为master死掉，而另选master的可能性较小；进而检查Ganglia集群监控，也没有发现异常的内网流量，故此原因可以排除。</p>\n<h4 id=\"2-节点负载：\" class=\"calibre15\">2. 节点负载：</h4>\n<p class=\"calibre12\">由于master节点与data节点都是混合在一起的，所以当工作节点的负载较大（确实也较大）时，导致对应的ES实例停止响应，而这台服务器如果正充当着master节点的身份，那么一部分节点就会认为这个master节点失效了，故重新选举新的节点，这时就出现了脑裂；同时由于data节点上ES进程占用的内存较大，较大规模的内存回收操作也能造成ES进程失去响应。所以，这个原因的可能性应该是最大的。</p>\n<h3 id=\"应对问题的办法：\" class=\"calibre9\">应对问题的办法：</h3>\n<h4 id=\"1-方法1：\" class=\"calibre15\">1. 方法1：</h4>\n<p class=\"calibre12\">对应于上面的分析，推测出原因应该是由于节点负载导致了master进程停止响应，继而导致了部分节点对于master的选择出现了分歧。为此，一个直观的解决方案便是将master节点与data节点分离。为此，我们添加了三台服务器进入ES集群，不过它们的角色只是master节点，不担任存储和搜索的角色，故它们是相对轻量级的进程。可以通过以下配置来限制其角色：</p>\n<pre class=\"calibre10\"><code class=\"pcalibre4 pcalibre3 calibre11\">node.master: true  \nnode.data: false\n</code></pre><p class=\"calibre12\">当然，其它的节点就不能再担任master了，把上面的配置反过来即可。这样就做到了将master节点与data节点分离。当然，为了使新加入的节点快速确定master位置，可以将data节点的默认的master发现方式有multicast修改为unicast：</p>\n<pre class=\"calibre10\"><code class=\"pcalibre4 pcalibre3 calibre11\">discovery.zen.ping.multicast.enabled: false  \ndiscovery.zen.ping.unicast.hosts: [\"master1\", \"master2\", \"master3\"]\n</code></pre><h4 id=\"还有两个直观的参数可以减缓脑裂问题的出现：\" class=\"calibre15\">还有两个直观的参数可以减缓脑裂问题的出现：</h4>\n<p class=\"calibre12\">discovery.zen.ping_timeout（默认值是3秒）：默认情况下，一个节点会认为，如果master节点在3秒之内没有应答，那么这个节点就是死掉了，而增加这个值，会增加节点等待响应的时间，从一定程度上会减少误判。</p>\n<p class=\"calibre12\">discovery.zen.minimum_master_nodes（默认是1）：这个参数控制的是，一个节点需要看到的具有master节点资格的最小数量，然后才能在集群中做操作。官方的推荐值是(N/2)+1，其中N是具有master资格的节点的数量（我们的情况是3，因此这个参数设置为2，但对于只有2个节点的情况，设置为2就有些问题了，一个节点DOWN掉后，你肯定连不上2台服务器了，这点需要注意）。</p>\n<p class=\"calibre12\">以上的解决方法只能是减缓这种现象的发生，并没有从根本上杜绝，但是毕竟是有帮助的，如果大家有其它更好的建议，欢迎探讨。</p>\n\n        </div>\n    \n</div>\n\n        \n    \n\n\n\n",
  "slug": 6588889,
  "title": "elasticsearch"
}